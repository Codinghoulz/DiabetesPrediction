{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "5d2f3e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#neural network\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "cdecf9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "df = pd.read_csv(r'C:\\Users\\piyus\\DiabetesPrediction\\data\\diabetes.csv')\n",
    "x = df.drop('Outcome', axis=1)  # All columns except the target\n",
    "y = df['Outcome']              # Only the target column\n",
    "x = x.to_numpy()\n",
    "y = y.to_numpy()\n",
    "x=x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "571678dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(X):\n",
    "    X_mean = np.mean(X, axis=0)\n",
    "    X_std = np.std(X, axis=0)\n",
    "    X_scaled = (X - X_mean) / X_std\n",
    "    return X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "952cd3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=scale(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "d3933971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(x,y,ratio,m):\n",
    "    #shuffle data\n",
    "    indices=np.arange(m)\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    x_shuffled= x[indices]\n",
    "    y_shuffled =y[indices]\n",
    "\n",
    "    #split data\n",
    "    train_size= int(ratio*m)\n",
    "    x_train,x_test=x_shuffled[:train_size] , x_shuffled[train_size:]\n",
    "    y_train,y_test=y_shuffled[:train_size] , y_shuffled[train_size:]\n",
    "\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "46b7a4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=x.shape[0]\n",
    "m=x.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "6b96065b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x.T,y.T,0.8,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "efb36d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=x_train.T,x_test.T,y_train.T,y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "eb9eab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "        return np.maximum(0,x)\n",
    "\n",
    "def sigmoid(z):\n",
    "        z = np.clip(z, -500, 500)  # or -100 to 100\n",
    "        y=1/(1+np.exp(-z))\n",
    "        return y\n",
    "activation=[relu,sigmoid]\n",
    "\n",
    "def reluder(z):\n",
    "        return (z > 0).astype(float)\n",
    "def sigder(z):\n",
    "        A = activation[1](z)\n",
    "        return A * (1 - A)\n",
    "derivatives=[reluder,sigder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "7e0a63bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(w,x,b,activation):\n",
    "    z=np.dot(w,x)+b\n",
    "    a=activation(z)\n",
    "    return a,z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "267aaa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_cost(y, y_hat):\n",
    "    m = y.shape[0]\n",
    "    epsilon = 1e-15  # To avoid log(0)\n",
    "    y_hat = np.clip(y_hat, epsilon, 1 - epsilon)\n",
    "    cost = - (1/m) * np.sum(\n",
    "        y * np.log(y_hat + epsilon) + (1 - y) * np.log(1 - y_hat + epsilon)\n",
    "    )\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "51309844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialiser(n_l,n_n,x):\n",
    "    \n",
    "    n_n.insert(0,x.shape[0])\n",
    "    weights = []\n",
    "    biases=[]\n",
    "    for i in range(n_l):\n",
    "        weights.append(np.random.rand(n_n[i+1],n_n[i] )*0.01 ) # Add random 2x2 arrays\n",
    "        biases.append(np.zeros((n_n[i+1],1)))\n",
    "    return weights,biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "aa551915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x,y,n_l,n_n,act,epochs=10000,alpha=0.05):\n",
    "    W,B=initialiser(n_l,n_n,x)\n",
    "    m=x.shape[1]\n",
    "    \n",
    "    #forward pass\n",
    "    for j in range (epochs):\n",
    "        input_data=x\n",
    "        A=[]\n",
    "        Z=[]\n",
    "        for i in range (n_l):\n",
    "            z, a = forward(W[i], input_data, B[i], activation[act[i]])\n",
    "            Z.append(z)\n",
    "            A.append(a)\n",
    "            input_data=A[i]\n",
    "        # backward pass\n",
    "        dA=[]\n",
    "        dZ=[]\n",
    "        dW=[]\n",
    "        dB=[]\n",
    "        da_o= - (np.divide(y, A[n_l-1]) - np.divide(1 - y, 1 - A[n_l-1]))\n",
    "        dA.append(da_o)\n",
    "        for i in range (n_l):     \n",
    "            dz = dA[i] * derivatives[act[n_l - 1 - i]](Z[n_l - 1 - i])\n",
    "            dZ.append(dz)\n",
    "            prev_A = A[(n_l - 2 - i)] if (n_l - 2 - i) >= 0 else x\n",
    "            dw = (1/m) * np.dot(dZ[i], prev_A.T)\n",
    "            dW.append(dw)\n",
    "            db = (1/m) * np.sum(dZ[i], axis=1, keepdims=True)\n",
    "            dB.append(db)\n",
    "            da = np.dot(W[n_l-1-i].T , dZ[i])\n",
    "            dA.append(da)\n",
    "        #updating params\n",
    "        for i in range(n_l):\n",
    "            W[i]=W[i]-alpha*dW[n_l-1-i]\n",
    "            B[i]=B[i]-alpha*dB[n_l-1-i]\n",
    "            \n",
    "        if (j%100==0):\n",
    "            cost=compute_cost(y,A[n_l-1])\n",
    "            print(f\"cost at iteration {j}:{cost}\")\n",
    "    return W,B,A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "a2a63fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "l=3\n",
    "neurons=[8,6,1]\n",
    "act=[0,0,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "473f66a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(W, B, x, act):\n",
    "    \"\"\"\n",
    "    Performs a forward pass using trained weights and returns the final activation (predicted probabilities).\n",
    "    \"\"\"\n",
    "    input_data = x\n",
    "    for i in range(len(W)):\n",
    "        z = np.dot(W[i], input_data) + B[i]\n",
    "        input_data = activation[act[i]](z)\n",
    "    return input_data  # final layer output (probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "af8685b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes accuracy between true labels and predicted probabilities.\n",
    "    \"\"\"\n",
    "    y_pred_labels = (y_pred > 0.5).astype(int)\n",
    "    return np.mean(y_pred_labels == y_true)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "02ecfbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost at iteration 0:11.54057708083656\n",
      "cost at iteration 100:11.575866669403348\n",
      "cost at iteration 200:11.575866669403348\n",
      "cost at iteration 300:11.575866669403348\n",
      "cost at iteration 400:11.575866669403348\n",
      "cost at iteration 500:11.575866669403348\n",
      "cost at iteration 600:11.575866669403348\n",
      "cost at iteration 700:11.575866669403348\n",
      "cost at iteration 800:11.575866669403348\n",
      "cost at iteration 900:11.575866669403348\n",
      "cost at iteration 1000:11.575866669403348\n",
      "cost at iteration 1100:11.575866669403348\n",
      "cost at iteration 1200:11.575866669403348\n",
      "cost at iteration 1300:11.575866669403348\n",
      "cost at iteration 1400:11.575866669403348\n",
      "cost at iteration 1500:11.575866669403348\n",
      "cost at iteration 1600:11.575866669403348\n",
      "cost at iteration 1700:11.575866669403348\n",
      "cost at iteration 1800:11.575866669403348\n",
      "cost at iteration 1900:11.575866669403348\n",
      "cost at iteration 2000:11.575866669403348\n",
      "cost at iteration 2100:11.575866669403348\n",
      "cost at iteration 2200:11.575866669403348\n",
      "cost at iteration 2300:11.575866669403348\n",
      "cost at iteration 2400:11.575866669403348\n",
      "cost at iteration 2500:11.575866669403348\n",
      "cost at iteration 2600:11.575866669403348\n",
      "cost at iteration 2700:11.575866669403348\n",
      "cost at iteration 2800:11.575866669403348\n",
      "cost at iteration 2900:11.575866669403348\n",
      "cost at iteration 3000:11.575866669403348\n",
      "cost at iteration 3100:11.575866669403348\n",
      "cost at iteration 3200:11.575866669403348\n",
      "cost at iteration 3300:11.575866669403348\n",
      "cost at iteration 3400:11.575866669403348\n",
      "cost at iteration 3500:11.575866669403348\n",
      "cost at iteration 3600:11.575866669403348\n",
      "cost at iteration 3700:11.575866669403348\n",
      "cost at iteration 3800:11.575866669403348\n",
      "cost at iteration 3900:11.575866669403348\n",
      "cost at iteration 4000:11.575866669403348\n",
      "cost at iteration 4100:11.575866669403348\n",
      "cost at iteration 4200:11.575866669403348\n",
      "cost at iteration 4300:11.575866669403348\n",
      "cost at iteration 4400:11.575866669403348\n",
      "cost at iteration 4500:11.575866669403348\n",
      "cost at iteration 4600:11.575866669403348\n",
      "cost at iteration 4700:11.575866669403348\n",
      "cost at iteration 4800:11.575866669403348\n",
      "cost at iteration 4900:11.575866669403348\n",
      "cost at iteration 5000:11.575866669403348\n",
      "cost at iteration 5100:11.575866669403348\n",
      "cost at iteration 5200:11.575866669403348\n",
      "cost at iteration 5300:11.575866669403348\n",
      "cost at iteration 5400:11.575866669403348\n",
      "cost at iteration 5500:11.575866669403348\n",
      "cost at iteration 5600:11.575866669403348\n",
      "cost at iteration 5700:11.575866669403348\n",
      "cost at iteration 5800:11.575866669403348\n",
      "cost at iteration 5900:11.575866669403348\n",
      "cost at iteration 6000:11.575866669403348\n",
      "cost at iteration 6100:11.575866669403348\n",
      "cost at iteration 6200:11.575866669403348\n",
      "cost at iteration 6300:11.575866669403348\n",
      "cost at iteration 6400:11.575866669403348\n",
      "cost at iteration 6500:11.575866669403348\n",
      "cost at iteration 6600:11.575866669403348\n",
      "cost at iteration 6700:11.575866669403348\n",
      "cost at iteration 6800:11.575866669403348\n",
      "cost at iteration 6900:11.575866669403348\n",
      "cost at iteration 7000:11.575866669403348\n",
      "cost at iteration 7100:11.575866669403348\n",
      "cost at iteration 7200:11.575866669403348\n",
      "cost at iteration 7300:11.575866669403348\n",
      "cost at iteration 7400:11.575866669403348\n",
      "cost at iteration 7500:11.575866669403348\n",
      "cost at iteration 7600:11.575866669403348\n",
      "cost at iteration 7700:11.575866669403348\n",
      "cost at iteration 7800:11.575866669403348\n",
      "cost at iteration 7900:11.575866669403348\n",
      "cost at iteration 8000:11.575866669403348\n",
      "cost at iteration 8100:11.575866669403348\n",
      "cost at iteration 8200:11.575866669403348\n",
      "cost at iteration 8300:11.575866669403348\n",
      "cost at iteration 8400:11.575866669403348\n",
      "cost at iteration 8500:11.575866669403348\n",
      "cost at iteration 8600:11.575866669403348\n",
      "cost at iteration 8700:11.575866669403348\n",
      "cost at iteration 8800:11.575866669403348\n",
      "cost at iteration 8900:11.575866669403348\n",
      "cost at iteration 9000:11.575866669403348\n",
      "cost at iteration 9100:11.575866669403348\n",
      "cost at iteration 9200:11.575866669403348\n",
      "cost at iteration 9300:11.575866669403348\n",
      "cost at iteration 9400:11.575866669403348\n",
      "cost at iteration 9500:11.575866669403348\n",
      "cost at iteration 9600:11.575866669403348\n",
      "cost at iteration 9700:11.575866669403348\n",
      "cost at iteration 9800:11.575866669403348\n",
      "cost at iteration 9900:11.575866669403348\n",
      "Accuracy: 0.6233766233766234\n"
     ]
    }
   ],
   "source": [
    "# After training\n",
    "W_trained, B_trained,a = model(x_train, y_train, l, neurons, act, epochs=10000, alpha=0.05)\n",
    "\n",
    "# Get predictions\n",
    "y_pred = predict(W_trained, B_trained, x_test, act)\n",
    "\n",
    "# Accuracy\n",
    "print(\"Accuracy:\", accuracy(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f22675",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e214b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
